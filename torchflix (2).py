# -*- coding: utf-8 -*-
"""torchflix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aum7tlMS6LzS2xsovCpn79hOX4j7BqZ4
"""

# Core library imports

import pandas as pd
import numpy as np
import ast
import io
from collections import Counter
import joblib

import torch
import torch.nn as nn
import torch.nn.functional as F

from sklearn.utils import Bunch
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MultiLabelBinarizer
from scipy.sparse import hstack


# Import CSVS
movies_metadata = pd.read_csv("C:/Users/adity/Downloads/datasets/movies_metadata.csv", low_memory=False)
credits = pd.read_csv("C:/Users/adity/Downloads/datasets/credits.csv")
keywords = pd.read_csv("C:/Users/adity/Downloads/datasets/keywords.csv")
ratings_small = pd.read_csv("C:/Users/adity/Downloads/datasets/ratings_small.csv")

#JSON Parser

def parse_json_list(s):
    try:
        return [x['name'] for x in ast.literal_eval(s)]
    except Exception:
        return []

# Applies JSON function to columns

movies_metadata["genre_list"] = movies_metadata["genres"].apply(parse_json_list)
credits['cast_list'] = credits['cast'].apply(parse_json_list)
keywords['keywords_list'] = keywords['keywords'].apply(parse_json_list)

# Drops original columns

movies_metadata.drop(columns=["genres"], inplace=True)
credits.drop(columns=["cast"], inplace=True)
keywords.drop(columns=["keywords"], inplace=True)

# Converts ID merge columns to numerics

movies_metadata['id'] = pd.to_numeric(movies_metadata['id'], errors='coerce')
credits['id'] = pd.to_numeric(credits['id'], errors='coerce')
keywords['id'] = pd.to_numeric(keywords['id'], errors='coerce')

# Changes name to movie id so merging is possible

movies_metadata.rename(columns={'id': 'movieId'}, inplace=True)
credits.rename(columns={'id': 'movieId'}, inplace=True)
keywords.rename(columns={'id': 'movieId'}, inplace=True)

# Merge Datasets on MovieID

df = movies_metadata[['movieId', 'title', 'genre_list']] \
     .merge(credits[['movieId', 'cast_list']], on='movieId') \
     .merge(keywords[['movieId', 'keywords_list']], on='movieId')

# Drops NA Rows

df.dropna(subset = ["genre_list", "cast_list", "keywords_list"], inplace = True)

# Drops empty lists

df = df[df['genre_list'].map(len) > 0]
df = df[df['cast_list'].map(len) > 0]
df = df[df['keywords_list'].map(len) > 0]

# Ensure all values are lists
def ensure_list(x):
    return x if isinstance(x, list) else []

df['genre_list'] = df['genre_list'].apply(ensure_list)
df['cast_list'] = df['cast_list'].apply(ensure_list)
df['keywords_list'] = df['keywords_list'].apply(ensure_list)

# Set Minimum actors, genres, keywords to be considered into the algo

min_actors = 20
min_genres   = 5
min_keywords = 5

# Calculate the counts how many times each item appears in the dataset

genre_counts = Counter([g for sub in df['genre_list'] for g in sub])
actor_counts = Counter([a for sub in df['cast_list'] for a in sub])
keyword_counts = Counter([k for sub in df['keywords_list'] for k in sub])

# Only keep actors that pass the minimum threshold

df['cast_list'] = df['cast_list'].apply(
    lambda lst: [a for a in lst if actor_counts[a] >= min_actors]
)
df['genre_list'] = df['genre_list'].apply(
    lambda lst: [g for g in lst if genre_counts[g] >= min_genres]
)
df['keywords_list'] = df['keywords_list'].apply(
    lambda lst: [k for k in lst if keyword_counts[k] >= min_keywords]
)

# Make sure there are no empty lists

df = df[df['genre_list'].map(len) > 0]
df = df[df['cast_list'].map(len) > 0]
df = df[df['keywords_list'].map(len) > 0]

# MultiLabelBinarizer setup

genre_mlb = MultiLabelBinarizer(sparse_output=True) # sparse only stores the non zero values: saves memory
actor_mlb = MultiLabelBinarizer(sparse_output=True)
keyword_mlb = MultiLabelBinarizer(sparse_output=True)


# Fit + transform

genre_matrix   = genre_mlb.fit_transform(df['genre_list'].tolist())
actor_matrix   = actor_mlb.fit_transform(df['cast_list'].tolist())
keyword_matrix = keyword_mlb.fit_transform(df['keywords_list'].tolist())

# Combine into single sparse matrix

movie_features = hstack([genre_matrix, actor_matrix, keyword_matrix])

# Sort training_data by movies we are still  u sing

ratings_train = ratings_small.copy()
ratings_train["liked"] = ratings_train["rating"] >= 4.0

# Sort uncommon movies out

valid_movie_ids = set(df['movieId'])
ratings_train = ratings_train[ratings_train['movieId'].isin(valid_movie_ids)]

# Create FinalDF

movie_feature_df = pd.DataFrame(movie_features.toarray())
movie_feature_df['movieId'] = df['movieId'].values
df1 = pd.merge(ratings_train, movie_feature_df, how = "left", on = "movieId")

# In Case Sparse is needed
#movie_feature_df = pd.DataFrame.sparse.from_spmatrix(movie_features)

# Begin training data
# Set X and y

X = df1.drop(columns=["userId", "movieId", "rating", "timestamp", "liked"])
y = df1["liked"]

# Use GPU

device = "cuda" if torch.cuda.is_available() else "cpu"

# Training Data

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Switch to GPU
X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)

X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)

# Define the machine learning model

class TorchFlix(nn.Module):
    def __init__(self, input_shape, hidden_units, output_units):
        super().__init__()
        self.layer_stack = nn.Sequential(
            nn.Flatten(),  # Just in case
            nn.Linear(input_shape, hidden_units),
            nn.BatchNorm1d(hidden_units),
            nn.ReLU(),
            nn.Dropout(0.1),

            nn.Linear(hidden_units, hidden_units // 2),
            nn.BatchNorm1d(hidden_units // 2),
            nn.ReLU(),
            nn.Dropout(0.1),

            nn.Linear(hidden_units // 2, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.1),

            nn.Linear(64, output_units)  # Final output layer (no sigmoid!)
        )

        # Weight initialization (Xavier/Glorot)
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)

    def forward(self, x):
        return self.layer_stack(x)

TorchFlixModel = TorchFlix(input_shape=X_train.shape[1], hidden_units=256, output_units=1).to(device)

loss_fn = nn.BCEWithLogitsLoss() # Binary Cross Entropy Loss - Loss = -[ y * log(p) + (1 - y) * log(1 - p) ] - best used for binary data
optimizer = torch.optim.Adam(TorchFlixModel.parameters(), lr=0.01) # Adaptive moment estimation -  uses momentum

# Training Loop

epochs = 300
for epoch in range(epochs):
  TorchFlixModel.train()

  y_pred = TorchFlixModel(X_train_tensor).squeeze()  # Squeeze if output is [batch_size, 1]
  loss = loss_fn(y_pred, y_train_tensor)

  optimizer.zero_grad() # Zero out gradients (slope of loss)
  loss.backward() # Back Propogation
  optimizer.step() #Update Parameters

  if epoch % 100 == 0:
    preds = (y_pred >= 0.5).float()

    # Compute training accuracy
    acc = (preds == y_train_tensor).float().mean()

    # Print both loss and accuracy
    print(f"Epoch {epoch}, Loss: {loss.item():.4f}, Accuracy: {acc:.4f}")

  if epoch % 50 == 0:
      TorchFlixModel.eval()
      with torch.no_grad():
          val_preds = TorchFlixModel(X_test_tensor).squeeze()
          val_probs = torch.sigmoid(val_preds)
          val_labels = (val_probs >= 0.5).float()
          val_acc = (val_labels == y_test_tensor).float().mean()
          print(f"[VALIDATION] Epoch {epoch}, Accuracy: {val_acc:.4f}")
      TorchFlixModel.train()

# Testing Loop

TorchFlixModel.eval()
with torch.no_grad():
    y_pred1 = TorchFlixModel(X_test_tensor).squeeze()
    y_pred_labels = (y_pred1 >= 0.5).float()
    acc = (y_pred_labels == y_test_tensor).float().mean()
    print(f"Test Accuracy: {acc:.4f}")

# Save trained model
TorchFlixModel.eval()  # Just in case
torch.save(TorchFlixModel.state_dict(), "torchflix_weights.pth")
print("Model weights saved as torchflix_weights.pth")

joblib.dump(genre_mlb, "genre_mlb.pkl")
joblib.dump(actor_mlb, "actor_mlb.pkl")
joblib.dump(keyword_mlb, "keyword_mlb.pkl")

